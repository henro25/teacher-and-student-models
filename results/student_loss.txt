Distilling Teacher Knowledge into a Single Student:
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Distill Loss: 4.8974, Accuracy: 0.4637
Distill Loss: 3.8757, Accuracy: 0.5970
Distill Loss: 3.5253, Accuracy: 0.6410
Distill Loss: 3.2854, Accuracy: 0.6767
Distill Loss: 3.1180, Accuracy: 0.6970
Distill Loss: 2.9876, Accuracy: 0.7131
Distill Loss: 2.8944, Accuracy: 0.7242
Distill Loss: 2.8034, Accuracy: 0.7376
Distill Loss: 2.7447, Accuracy: 0.7435
Distill Loss: 2.6824, Accuracy: 0.7511
Distill Loss: 2.6363, Accuracy: 0.7561
Distill Loss: 2.5848, Accuracy: 0.7627
Distill Loss: 2.5426, Accuracy: 0.7678
Distill Loss: 2.5026, Accuracy: 0.7719
Distill Loss: 2.4649, Accuracy: 0.7768
Distill Loss: 2.4339, Accuracy: 0.7820
Distill Loss: 2.4088, Accuracy: 0.7841
Distill Loss: 2.3648, Accuracy: 0.7884
Distill Loss: 2.3495, Accuracy: 0.7898
Distill Loss: 2.3174, Accuracy: 0.7932
Distill Loss: 2.2834, Accuracy: 0.7995
Distill Loss: 2.2693, Accuracy: 0.8007
Distill Loss: 2.2455, Accuracy: 0.8026
Distill Loss: 2.2329, Accuracy: 0.8016
Distill Loss: 2.2139, Accuracy: 0.8068
Distill Loss: 2.2049, Accuracy: 0.8075
Distill Loss: 2.1749, Accuracy: 0.8105
Distill Loss: 2.1606, Accuracy: 0.8125
Distill Loss: 2.1360, Accuracy: 0.8112
Distill Loss: 2.1380, Accuracy: 0.8152
Distill Loss: 2.1163, Accuracy: 0.8185
Distill Loss: 2.1080, Accuracy: 0.8191
Distill Loss: 2.1020, Accuracy: 0.8187
Distill Loss: 2.0843, Accuracy: 0.8211
Distill Loss: 2.0625, Accuracy: 0.8245
Distill Loss: 2.0506, Accuracy: 0.8247
Distill Loss: 2.0349, Accuracy: 0.8255
Distill Loss: 2.0289, Accuracy: 0.8263
Distill Loss: 2.0232, Accuracy: 0.8263
Distill Loss: 2.0072, Accuracy: 0.8291
Distill Loss: 2.0021, Accuracy: 0.8305
Distill Loss: 1.9772, Accuracy: 0.8322
Distill Loss: 1.9888, Accuracy: 0.8305
Distill Loss: 1.9655, Accuracy: 0.8347
Distill Loss: 1.9684, Accuracy: 0.8321
Distill Loss: 1.9629, Accuracy: 0.8349
Distill Loss: 1.9520, Accuracy: 0.8358
Distill Loss: 1.9470, Accuracy: 0.8382
Distill Loss: 1.9290, Accuracy: 0.8374
Distill Loss: 1.9264, Accuracy: 0.8387
Single Student saved to student_1.pth

Evaluating the Single Student:
Unsupported operator aten::max_pool2d encountered 2 time(s)
Single Student Results:
Loss: 0.5772, Accuracy: 0.8242
Latency per Image: 0.001170 secs
FLOPs per Image: 0.10 MFLOPs