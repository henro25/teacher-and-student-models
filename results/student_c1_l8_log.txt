Distilling Teacher Knowledge into a Single Student:
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Distill Loss: 3.1915, Accuracy: 0.4707
Distill Loss: 2.0442, Accuracy: 0.6153
Distill Loss: 1.6644, Accuracy: 0.6689
Distill Loss: 1.4342, Accuracy: 0.7016
Distill Loss: 1.3028, Accuracy: 0.7216
Distill Loss: 1.1928, Accuracy: 0.7378
Distill Loss: 1.0997, Accuracy: 0.7518
Distill Loss: 1.0415, Accuracy: 0.7596
Distill Loss: 0.9855, Accuracy: 0.7692
Distill Loss: 0.9427, Accuracy: 0.7776
Distill Loss: 0.9168, Accuracy: 0.7814
Distill Loss: 0.8796, Accuracy: 0.7863
Distill Loss: 0.8492, Accuracy: 0.7923
Distill Loss: 0.8233, Accuracy: 0.7970
Distill Loss: 0.8037, Accuracy: 0.8023
Distill Loss: 0.7804, Accuracy: 0.8030
Distill Loss: 0.7633, Accuracy: 0.8061
Distill Loss: 0.7509, Accuracy: 0.8091
Distill Loss: 0.7385, Accuracy: 0.8115
Distill Loss: 0.7257, Accuracy: 0.8162
Single Student saved to student_c1_l8.pth

Evaluating the Single Student:
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 2 time(s)
Single Student Results:
Loss: 0.5789, Accuracy: 0.8103
Latency per Image: 0.000582 secs
FLOPs per Image: 0.22 MFLOPs

Joint Training of Mixture of Experts (MoE):
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1/20 - MoE Loss: 0.5554, Accuracy: 0.8121
Epoch 2/20 - MoE Loss: 0.5291, Accuracy: 0.8199
Epoch 3/20 - MoE Loss: 0.5041, Accuracy: 0.8275
Epoch 4/20 - MoE Loss: 0.4881, Accuracy: 0.8329
Epoch 5/20 - MoE Loss: 0.4532, Accuracy: 0.8462
Epoch 6/20 - MoE Loss: 0.4378, Accuracy: 0.8510
Epoch 7/20 - MoE Loss: 0.4165, Accuracy: 0.8580
Epoch 8/20 - MoE Loss: 0.3889, Accuracy: 0.8679
Epoch 9/20 - MoE Loss: 0.3669, Accuracy: 0.8745
Epoch 10/20 - MoE Loss: 0.3456, Accuracy: 0.8823
Epoch 11/20 - MoE Loss: 0.3239, Accuracy: 0.8902
Epoch 12/20 - MoE Loss: 0.3033, Accuracy: 0.8964
Epoch 13/20 - MoE Loss: 0.2827, Accuracy: 0.9039
Epoch 14/20 - MoE Loss: 0.2622, Accuracy: 0.9109
Epoch 15/20 - MoE Loss: 0.2460, Accuracy: 0.9155
Epoch 16/20 - MoE Loss: 0.2323, Accuracy: 0.9208
Epoch 17/20 - MoE Loss: 0.2225, Accuracy: 0.9238
Epoch 18/20 - MoE Loss: 0.2155, Accuracy: 0.9265
Epoch 19/20 - MoE Loss: 0.2090, Accuracy: 0.9292
Epoch 20/20 - MoE Loss: 0.2075, Accuracy: 0.9304

Evaluating Mixture of Experts (MoE):
WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 2 time(s)
WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
gating_net, gating_net.conv_layers, gating_net.conv_layers.0, gating_net.conv_layers.1, gating_net.conv_layers.2, gating_net.conv_layers.3, gating_net.conv_layers.4, gating_net.conv_layers.5, gating_net.conv_layers.6, gating_net.conv_layers.7, gating_net.fc_layers, gating_net.fc_layers.0, gating_net.fc_layers.1, gating_net.fc_layers.2, gating_net.fc_layers.3, gating_net.fc_layers.4, students.1, students.1.network, students.1.network.0, students.1.network.1, students.1.network.2, students.1.network.3, students.1.network.4, students.1.network.5, students.1.network.6, students.1.network.7, students.1.network.8, students.1.network.9, students.2, students.2.network, students.2.network.0, students.2.network.1, students.2.network.2, students.2.network.3, students.2.network.4, students.2.network.5, students.2.network.6, students.2.network.7, students.2.network.8, students.2.network.9
MoE Results:
Loss: 0.4993, Accuracy: 0.8531
Latency per Image: 0.001069 secs
FLOPs per Image: 0.22 MFLOPs