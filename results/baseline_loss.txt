Joint Training of Mixture of Experts (MoE):
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1/50 - MoE Loss: 0.5453, Accuracy: 0.8199
Epoch 2/50 - MoE Loss: 0.5271, Accuracy: 0.8219
Epoch 3/50 - MoE Loss: 0.5208, Accuracy: 0.8244
Epoch 4/50 - MoE Loss: 0.5078, Accuracy: 0.8287
Epoch 5/50 - MoE Loss: 0.5016, Accuracy: 0.8309
Epoch 6/50 - MoE Loss: 0.4929, Accuracy: 0.8335
Epoch 7/50 - MoE Loss: 0.4837, Accuracy: 0.8353
Epoch 8/50 - MoE Loss: 0.4731, Accuracy: 0.8379
Epoch 9/50 - MoE Loss: 0.4737, Accuracy: 0.8393
Epoch 10/50 - MoE Loss: 0.4633, Accuracy: 0.8415
Epoch 11/50 - MoE Loss: 0.4523, Accuracy: 0.8452
Epoch 12/50 - MoE Loss: 0.4454, Accuracy: 0.8492
Epoch 13/50 - MoE Loss: 0.4326, Accuracy: 0.8528
Epoch 14/50 - MoE Loss: 0.4246, Accuracy: 0.8560
Epoch 15/50 - MoE Loss: 0.4240, Accuracy: 0.8567
Epoch 16/50 - MoE Loss: 0.4114, Accuracy: 0.8593
Epoch 17/50 - MoE Loss: 0.3986, Accuracy: 0.8636
Epoch 18/50 - MoE Loss: 0.3970, Accuracy: 0.8650
Epoch 19/50 - MoE Loss: 0.3886, Accuracy: 0.8656
Epoch 20/50 - MoE Loss: 0.3800, Accuracy: 0.8699
Epoch 21/50 - MoE Loss: 0.3806, Accuracy: 0.8698
Epoch 22/50 - MoE Loss: 0.3637, Accuracy: 0.8756
Epoch 23/50 - MoE Loss: 0.3571, Accuracy: 0.8772
Epoch 24/50 - MoE Loss: 0.3447, Accuracy: 0.8846
Epoch 25/50 - MoE Loss: 0.3391, Accuracy: 0.8833
Epoch 26/50 - MoE Loss: 0.3330, Accuracy: 0.8855
Epoch 27/50 - MoE Loss: 0.3233, Accuracy: 0.8888
Epoch 28/50 - MoE Loss: 0.3191, Accuracy: 0.8907
Epoch 29/50 - MoE Loss: 0.3111, Accuracy: 0.8923
Epoch 30/50 - MoE Loss: 0.3072, Accuracy: 0.8936
Epoch 31/50 - MoE Loss: 0.2949, Accuracy: 0.8984
Epoch 32/50 - MoE Loss: 0.2861, Accuracy: 0.9024
Epoch 33/50 - MoE Loss: 0.2840, Accuracy: 0.9023
Epoch 34/50 - MoE Loss: 0.2770, Accuracy: 0.9053
Epoch 35/50 - MoE Loss: 0.2658, Accuracy: 0.9098
Epoch 36/50 - MoE Loss: 0.2648, Accuracy: 0.9097
Epoch 37/50 - MoE Loss: 0.2608, Accuracy: 0.9087
Epoch 38/50 - MoE Loss: 0.2539, Accuracy: 0.9129
Epoch 39/50 - MoE Loss: 0.2471, Accuracy: 0.9159
Epoch 40/50 - MoE Loss: 0.2433, Accuracy: 0.9166
Epoch 41/50 - MoE Loss: 0.2461, Accuracy: 0.9143
Epoch 42/50 - MoE Loss: 0.2371, Accuracy: 0.9191
Epoch 43/50 - MoE Loss: 0.2351, Accuracy: 0.9208
Epoch 44/50 - MoE Loss: 0.2323, Accuracy: 0.9203
Epoch 45/50 - MoE Loss: 0.2316, Accuracy: 0.9216
Epoch 46/50 - MoE Loss: 0.2306, Accuracy: 0.9200
Epoch 47/50 - MoE Loss: 0.2309, Accuracy: 0.9212
Epoch 48/50 - MoE Loss: 0.2265, Accuracy: 0.9228
Epoch 49/50 - MoE Loss: 0.2321, Accuracy: 0.9216
Epoch 50/50 - MoE Loss: 0.2247, Accuracy: 0.9225

Evaluating Mixture of Experts (MoE):
Unsupported operator aten::max_pool2d encountered 2 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
gating_net, gating_net.conv_layers, gating_net.conv_layers.0, gating_net.conv_layers.1, gating_net.conv_layers.2, gating_net.conv_layers.3, gating_net.conv_layers.4, gating_net.conv_layers.5, gating_net.conv_layers.6, gating_net.conv_layers.7, gating_net.fc_layers, gating_net.fc_layers.0, gating_net.fc_layers.1, gating_net.fc_layers.2, gating_net.fc_layers.3, gating_net.fc_layers.4, students.1, students.1.network, students.1.network.0, students.1.network.1, students.1.network.2, students.1.network.3, students.1.network.4, students.1.network.5, students.1.network.6, students.1.network.7, students.1.network.8, students.1.network.9, students.2, students.2.network, students.2.network.0, students.2.network.1, students.2.network.2, students.2.network.3, students.2.network.4, students.2.network.5, students.2.network.6, students.2.network.7, students.2.network.8, students.2.network.9, students.3, students.3.network, students.3.network.0, students.3.network.1, students.3.network.2, students.3.network.3, students.3.network.4, students.3.network.5, students.3.network.6, students.3.network.7, students.3.network.8, students.3.network.9
MoE Results:
Loss: 0.5560, Accuracy: 0.8379
Latency per Image: 0.000924 secs
FLOPs per Image: 0.10 MFLOPs