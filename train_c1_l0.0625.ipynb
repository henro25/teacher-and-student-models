{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/henro25/teacher-and-student-models.git\n",
    "!mv /content/teacher-and-student-models/* /content\n",
    "!rm -r /content/teacher-and-student-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import AdamW\n",
    "from torchvision.models import resnet18\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "class Config:\n",
    "    in_channels = 3\n",
    "    num_classes = 10\n",
    "    batch_size = 64\n",
    "    lr = 1e-3\n",
    "    epochs = 20\n",
    "    num_students = 3\n",
    "    num_big_classes = num_students\n",
    "    hidden_dim = 256\n",
    "    #temperature = 3.0\n",
    "    temperature = 5.0\n",
    "    alpha = 0.7\n",
    "    teacher_model_path = \"resnet18_cifar10_tailored_epoch20.pth\"\n",
    "    student_model_path = \"student_{}.pth\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "class ResNet18CIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18CIFAR10, self).__init__()\n",
    "        self.model = resnet18(pretrained=False)  # Load base ResNet-18\n",
    "        self.model.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )  # Replace first conv layer\n",
    "        self.model.maxpool = nn.Identity()  # Remove MaxPooling layer\n",
    "        self.model.fc = nn.Linear(512, num_classes)  # Adjust fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Data Loaders with Data Augmentation\n",
    "def get_data_loaders():\n",
    "    # Training transformations (with augmentations)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Mean and std of CIFAR-10\n",
    "    ])\n",
    "\n",
    "    # Test transformations (without augmentations)\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Same mean and std as training set\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8 * 8 * 64, config.hidden_dim // 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_dim // 16, config.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, num_students, input_dim):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        # Convolutional layers to extract spatial features\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Output: 32 x 32 x 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                          # Output: 16 x 16 x 32\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Output: 16 x 16 x 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                          # Output: 8 x 8 x 64\n",
    "        )\n",
    "        # Fully connected layers to produce routing probabilities\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8 * 8 * 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_students)\n",
    "        )\n",
    "        self.temperature = 5.0  # High initial temperature for exploration\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to have shape [batch_size, 3, 32, 32]\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "        features = self.conv_layers(x)\n",
    "        logits = self.fc_layers(features)\n",
    "        # Apply temperature scaling to logits before softmax\n",
    "        return F.softmax(logits / self.temperature, dim=1)\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, students, gating_net):\n",
    "        super(MoE, self).__init__()\n",
    "        self.students = nn.ModuleList(students)\n",
    "        self.gating_net = gating_net\n",
    "\n",
    "    def forward(self, x, return_router_assignments=False):\n",
    "        batch_size = x.size(0)\n",
    "        gating_probs = self.gating_net(x.view(batch_size, -1))  # Router probabilities\n",
    "        best_experts = gating_probs.argmax(dim=1)  # Selected experts for each input\n",
    "\n",
    "        outputs = torch.zeros(batch_size, self.students[0].network[-1].out_features).to(x.device)\n",
    "        for i, expert_idx in enumerate(best_experts):\n",
    "            outputs[i] = self.students[expert_idx](x[i].unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        if return_router_assignments:\n",
    "            return outputs, best_experts\n",
    "        return outputs\n",
    "\n",
    "def distill_teacher_to_student(teacher, student, loader, optimizer, criterion, device):\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            teacher_soft = F.softmax(teacher_outputs / config.temperature, dim=1)\n",
    "\n",
    "        student_outputs = student(inputs)\n",
    "        student_soft = F.log_softmax(student_outputs / config.temperature, dim=1)\n",
    "\n",
    "        distill_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (config.temperature ** 2)\n",
    "        hard_loss = F.cross_entropy(student_outputs, targets)\n",
    "        loss = config.alpha * distill_loss + (1 - config.alpha) * hard_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (student_outputs.argmax(1) == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "    accuracy = correct / total_samples\n",
    "    print(f\"Distill Loss: {total_loss / len(loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def create_big_class_map_from_teacher(teacher, data_loader, num_big_classes, device):\n",
    "    \"\"\"\n",
    "    Create a big class map by clustering the teacher's logits.\n",
    "    \"\"\"\n",
    "    teacher.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            logits = teacher(inputs).cpu().numpy()\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate class embeddings by averaging teacher logits for each original class\n",
    "    class_embeddings = {c: all_logits[all_labels == c].mean(axis=0) for c in np.unique(all_labels)}\n",
    "    class_features = np.array([class_embeddings[c] for c in sorted(class_embeddings)])\n",
    "\n",
    "    # Cluster the class embeddings into big classes\n",
    "    kmeans = KMeans(n_clusters=num_big_classes, random_state=42).fit(class_features)\n",
    "    big_class_map = {c: cluster for c, cluster in zip(sorted(class_embeddings), kmeans.labels_)}\n",
    "\n",
    "    return big_class_map\n",
    "\n",
    "def balance_big_class_map(big_class_map):\n",
    "    total_counts = sum(big_class_map.values())\n",
    "    for class_idx in big_class_map:\n",
    "        big_class_map[class_idx] /= total_counts  # Normalize probabilities\n",
    "    return big_class_map\n",
    "\n",
    "def distill_teacher_to_router_with_clusters(\n",
    "    teacher, router, loader, optimizer, device, big_class_map, epochs=5, use_combined_loss=True\n",
    "):\n",
    "    teacher.eval()\n",
    "    router.train()\n",
    "\n",
    "    # Create reverse mapping from class to cluster\n",
    "    class_to_cluster = {c: cluster for c, cluster in big_class_map.items()}\n",
    "\n",
    "    # Define the cosine annealing scheduler\n",
    "    scheduler_router = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_assignments = 0\n",
    "        total_samples = 0\n",
    "        alpha = epoch / epochs if use_combined_loss else 1.0  # Linearly increase soft loss weight\n",
    "\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Compute cluster probabilities from teacher logits\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "                teacher_probs = F.softmax(teacher_logits / config.temperature, dim=1)\n",
    "                cluster_probs = torch.zeros(inputs.size(0), config.num_big_classes, device=device)\n",
    "                for cls, cluster in class_to_cluster.items():\n",
    "                    cluster_probs[:, cluster] += teacher_probs[:, cls]\n",
    "\n",
    "                # Compute hard cluster assignments for optional hard loss\n",
    "                hard_labels = cluster_probs.argmax(dim=1)\n",
    "\n",
    "            # Router outputs (probability of each cluster)\n",
    "            router_logits = router(inputs)\n",
    "            router_probs = F.softmax(router_logits, dim=1)\n",
    "\n",
    "            # Compute soft label loss (distillation)\n",
    "            soft_loss = F.kl_div(\n",
    "                F.log_softmax(router_logits / config.temperature, dim=1),\n",
    "                cluster_probs,\n",
    "                reduction='batchmean'\n",
    "            ) * (config.temperature ** 2)\n",
    "\n",
    "            # Compute hard label loss (if enabled)\n",
    "            hard_loss = F.cross_entropy(router_logits, hard_labels) if use_combined_loss else 0.0\n",
    "\n",
    "            # Combine losses\n",
    "            total_loss_batch = alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "\n",
    "            # # Diversity loss (optional)\n",
    "            # router_usage = router_probs.mean(dim=0)  # Average over the batch\n",
    "            # diversity_loss = torch.sum((router_usage - 1 / config.num_students) ** 2)\n",
    "\n",
    "            # # Add diversity loss to total loss\n",
    "            # total_loss_batch += 0.01 * diversity_loss\n",
    "\n",
    "            # Backprop and optimization step\n",
    "            optimizer.zero_grad()\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += total_loss_batch.item()\n",
    "            predicted_clusters = router_probs.argmax(dim=1)\n",
    "            ground_truth_clusters = torch.tensor(\n",
    "                [class_to_cluster[label.item()] for label in targets], device=device\n",
    "            )\n",
    "            correct_assignments += (predicted_clusters == ground_truth_clusters).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler_router.step()\n",
    "\n",
    "        # Log metrics\n",
    "        router_accuracy = correct_assignments / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Router Loss: {total_loss / len(loader):.4f}, Accuracy: {router_accuracy:.4f}\")\n",
    "\n",
    "    return router\n",
    "\n",
    "def evaluate_with_metrics(model, loader, device, description=\"Model\"):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss, correct = 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            if i == 0:\n",
    "                start_time = time.time()\n",
    "\n",
    "                flops_input = inputs[:1].to(device)\n",
    "                flops_analysis = FlopCountAnalysis(model, flops_input)\n",
    "                flops_per_image = flops_analysis.total() / batch_size\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                latency = (end_time - start_time) / batch_size\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == targets).sum().item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "    accuracy = correct / total_samples\n",
    "    print(f\"{description} Results:\")\n",
    "    print(f\"Loss: {total_loss / len(loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Latency per Image: {latency:.6f} secs\")\n",
    "    print(f\"FLOPs per Image: {flops_per_image / 1e6:.2f} MFLOPs\")\n",
    "\n",
    "    return total_loss, accuracy, latency, flops_per_image\n",
    "\n",
    "def visualize_specialization(moe_model, loader, device, num_classes, num_students):\n",
    "    moe_model.eval()\n",
    "    class_prob_tracker = defaultdict(lambda: torch.zeros(num_students, device=device))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            router_probs = moe_model.gating_net(inputs.view(inputs.size(0), -1))\n",
    "\n",
    "            for class_id in range(num_classes):\n",
    "                class_indices = (targets == class_id)\n",
    "                if class_indices.sum() > 0:\n",
    "                    class_prob_tracker[class_id] += router_probs[class_indices].mean(dim=0)\n",
    "\n",
    "    # Convert class probabilities to a NumPy array\n",
    "    data = torch.stack([class_prob_tracker[c] for c in range(num_classes)]).cpu().numpy()\n",
    "\n",
    "    # Visualization\n",
    "    bar_width = 0.75\n",
    "    x_indices = np.arange(num_classes)\n",
    "    student_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bottom_values = np.zeros(num_classes)\n",
    "\n",
    "    for student_id in range(num_students):\n",
    "        student_probs = data[:, student_id]\n",
    "        ax.bar(\n",
    "            x_indices,\n",
    "            student_probs,\n",
    "            bar_width,\n",
    "            bottom=bottom_values,\n",
    "            color=student_colors[student_id % len(student_colors)],\n",
    "            label=f\"Student {student_id}\",\n",
    "            alpha=0.9\n",
    "        )\n",
    "        bottom_values += student_probs\n",
    "\n",
    "    ax.set_xlabel(\"Classes\")\n",
    "    ax.set_ylabel(\"Router Probability\")\n",
    "    ax.set_title(\"Class Specialization Across Students\")\n",
    "    ax.set_xticks(x_indices)\n",
    "    ax.set_xticklabels([f\"Class {i}\" for i in range(num_classes)])\n",
    "    ax.legend(title=\"Students\", loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(\"specialization.png\")\n",
    "\n",
    "\n",
    "def train_moe(moe_model, train_loader, optimizer, criterion, device, epochs=10):\n",
    "    moe_model.train()\n",
    "\n",
    "    # Define the cosine annealing scheduler\n",
    "    scheduler_moe = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, router_assignments = moe_model(inputs, return_router_assignments=True)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler_moe.step()\n",
    "\n",
    "        # Logging epoch metrics\n",
    "        accuracy = correct / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - MoE Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return moe_model\n",
    "\n",
    "def visualize_router_assignments(router, loader, num_classes, num_students, device):\n",
    "    router.eval()\n",
    "    assignment_counts = torch.zeros(num_classes, num_students, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            router_probs = router(inputs.view(inputs.size(0), -1))\n",
    "\n",
    "            for class_id in range(num_classes):\n",
    "                class_indices = (targets == class_id)\n",
    "                if class_indices.sum() > 0:\n",
    "                    assignment_counts[class_id] += router_probs[class_indices].sum(dim=0)\n",
    "\n",
    "    # Normalize counts for visualization\n",
    "    assignment_counts = assignment_counts.cpu().numpy()\n",
    "    assignment_counts /= assignment_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Plot\n",
    "    bar_width = 0.75\n",
    "    x_indices = np.arange(num_classes)\n",
    "    student_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "\n",
    "    plt.figure(figsize=(14, 8))  # Increase figure size for better visibility\n",
    "    bottom_values = np.zeros(num_classes)\n",
    "\n",
    "    for student_idx in range(num_students):\n",
    "        student_probs = assignment_counts[:, student_idx]\n",
    "        plt.bar(\n",
    "            x_indices,\n",
    "            student_probs,\n",
    "            bar_width,\n",
    "            bottom=bottom_values,\n",
    "            color=student_colors[student_idx % len(student_colors)],\n",
    "            label=f\"Student {student_idx}\",\n",
    "            alpha=0.9\n",
    "        )\n",
    "        bottom_values += student_probs  # Stack bars vertically\n",
    "\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Router Assignment Probabilities\")\n",
    "    plt.title(\"Router Assignment Probabilities per Class\")\n",
    "    plt.xticks(x_indices, [f\"Class {i}\" for i in range(num_classes)], rotation=45)\n",
    "    plt.ylim(0, 1)  # Ensure the y-axis always ranges from 0 to 1\n",
    "    plt.legend(title=\"Students\", loc=\"upper right\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add a grid for better readability\n",
    "    plt.tight_layout()  # Adjust layout to fit everything properly\n",
    "    plt.show()\n",
    "    plt.savefig(\"router_assignments.png\")\n",
    "\n",
    "def train_teacher(teacher, train_loader, test_loader, device, epochs=10, lr=1e-3):\n",
    "    teacher.train()\n",
    "    optimizer = AdamW(teacher.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total_samples = 0, 0, 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = teacher(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "        accuracy = correct / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nEvaluating Teacher Model:\")\n",
    "    evaluate_with_metrics(teacher, test_loader, device, description=\"Teacher\")\n",
    "    return teacher\n",
    "\n",
    "def fine_tune_router_with_hard_labels(router, train_loader, device, big_class_map, epochs=5, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Fine-tune the router with hard cluster labels, using cosine annealing.\n",
    "    \n",
    "    Args:\n",
    "        router (nn.Module): The router network to fine-tune.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        device (torch.device): Device to run training on.\n",
    "        big_class_map (dict): Mapping from class labels to cluster labels.\n",
    "        epochs (int): Number of fine-tuning epochs.\n",
    "        lr (float): Initial learning rate.\n",
    "    \"\"\"\n",
    "    # Reverse mapping from class labels to cluster labels\n",
    "    class_to_cluster = {c: cluster for c, cluster in big_class_map.items()}\n",
    "    \n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = AdamW(router.parameters(), lr=lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)  # Add Cosine Annealing\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Fine-tune the router\n",
    "    for epoch in range(epochs):\n",
    "        router.train()\n",
    "        total_loss = 0\n",
    "        correct_assignments = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Map class labels to cluster labels\n",
    "            cluster_labels = torch.tensor(\n",
    "                [class_to_cluster[label.item()] for label in targets],\n",
    "                device=device,\n",
    "                dtype=torch.long  # Ensure the target tensor is of type long\n",
    "            )\n",
    "            \n",
    "            # Forward pass through the router\n",
    "            optimizer.zero_grad()\n",
    "            router_logits = router(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(router_logits, cluster_labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            predicted_clusters = router_logits.argmax(dim=1)\n",
    "            correct_assignments += (predicted_clusters == cluster_labels).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "        \n",
    "        # Step the cosine annealing scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log metrics for the epoch\n",
    "        accuracy = correct_assignments / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Fine-tuning Loss: {total_loss / len(train_loader):.4f}, \"\n",
    "              f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "def evaluate_router_with_cluster_labels(router, loader, device, big_class_map):\n",
    "    \"\"\"\n",
    "    Evaluate the router's accuracy based on cluster labels.\n",
    "    \n",
    "    Args:\n",
    "        router (nn.Module): The trained router.\n",
    "        loader (DataLoader): DataLoader for the dataset to evaluate.\n",
    "        device (torch.device): Device to run evaluation on.\n",
    "        big_class_map (dict): Mapping from class labels to cluster labels.\n",
    "    \"\"\"\n",
    "    router.eval()\n",
    "    class_to_cluster = {c: cluster for c, cluster in big_class_map.items()}\n",
    "    \n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Map class labels to cluster labels\n",
    "            cluster_labels = torch.tensor(\n",
    "                [class_to_cluster[label.item()] for label in targets],\n",
    "                device=device,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            # Forward pass through the router\n",
    "            router_logits = router(inputs)\n",
    "            predicted_clusters = router_logits.argmax(dim=1)\n",
    "            \n",
    "            # Compare predictions to ground truth cluster labels\n",
    "            correct_predictions += (predicted_clusters == cluster_labels).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    print(f\"Router Accuracy Based on Cluster Labels: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_router_cluster_accuracies(router, loader, device, big_class_map):\n",
    "    \"\"\"\n",
    "    Evaluate the router's accuracy for each cluster individually.\n",
    "    \n",
    "    Args:\n",
    "        router (nn.Module): The trained router.\n",
    "        loader (DataLoader): DataLoader for the validation dataset.\n",
    "        device (torch.device): Device to run evaluation on.\n",
    "        big_class_map (dict): Mapping from class labels to cluster labels.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Cluster-wise accuracy metrics.\n",
    "    \"\"\"\n",
    "    router.eval()\n",
    "    class_to_cluster = {c: cluster for c, cluster in big_class_map.items()}\n",
    "    \n",
    "    # Initialize tracking metrics for each cluster\n",
    "    cluster_correct = defaultdict(int)\n",
    "    cluster_total = defaultdict(int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Map class labels to cluster labels\n",
    "            cluster_labels = torch.tensor(\n",
    "                [class_to_cluster[label.item()] for label in targets],\n",
    "                device=device,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            # Forward pass through the router\n",
    "            router_logits = router(inputs)\n",
    "            predicted_clusters = router_logits.argmax(dim=1)\n",
    "            \n",
    "            # Update metrics for each cluster\n",
    "            for i in range(len(cluster_labels)):\n",
    "                cluster = cluster_labels[i].item()\n",
    "                cluster_total[cluster] += 1\n",
    "                if predicted_clusters[i] == cluster_labels[i]:\n",
    "                    cluster_correct[cluster] += 1\n",
    "\n",
    "    # Calculate accuracy for each cluster\n",
    "    cluster_accuracies = {}\n",
    "    for cluster, correct_count in cluster_correct.items():\n",
    "        accuracy = correct_count / cluster_total[cluster] if cluster_total[cluster] > 0 else 0.0\n",
    "        cluster_accuracies[cluster] = accuracy\n",
    "        print(f\"Cluster {cluster}: Accuracy = {accuracy:.4f} ({correct_count}/{cluster_total[cluster]})\")\n",
    "    \n",
    "    return cluster_accuracies\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "\n",
    "    # Step 1: Initialize and train the teacher model\n",
    "    # teacher = TeacherModel().to(device)\n",
    "    # print(\"Training the Teacher Model:\")\n",
    "    # teacher = train_teacher(teacher, train_loader, test_loader, device, epochs=config.epochs, lr=config.lr)\n",
    "\n",
    "    # teacher = TeacherModel().to(device)\n",
    "    # teacher.load_state_dict(torch.load(config.teacher_model_path, map_location=device))\n",
    "    # teacher.eval()\n",
    "\n",
    "    teacher = ResNet18CIFAR10().to(device)\n",
    "    teacher.load_state_dict(torch.load(\"resnet18_cifar10_tailored_epoch20.pth\"))\n",
    "    teacher.to(device)\n",
    "\n",
    "    # print(f\"\\nEvaluating Teacher:\")\n",
    "    # evaluate_with_metrics(teacher, test_loader, device, description=\"Teacher\")\n",
    "\n",
    "    # Step 2: Create the big class map from the teacher's logits\n",
    "    print(\"\\nCreating Big Class Map from Teacher:\")\n",
    "    big_class_map = create_big_class_map_from_teacher(teacher, train_loader, config.num_big_classes, device)\n",
    "    print(\"Big Class Map:\", big_class_map)\n",
    "\n",
    "    # # Optional: Balance the big class map\n",
    "    # print(\"\\nBalancing Big Class Map:\")\n",
    "    # big_class_map = balance_big_class_map(big_class_map)\n",
    "    # print(\"Balanced Big Class Map:\", big_class_map)\n",
    "\n",
    "    # Step 3: Initialize the router\n",
    "    router = GatingNetwork(num_students=config.num_students, input_dim=3 * 32 * 32).to(device)\n",
    "    optimizer_router = AdamW(router.parameters(), lr=config.lr)\n",
    "    # optimizer_router = AdamW(router.parameters(), lr=config.lr, weight_decay=0.01)  # Add weight decay\n",
    "\n",
    "    # Step 4: Initialize the students\n",
    "    print(\"\\nInitializing Students:\")\n",
    "    students = [StudentModel().to(device) for _ in range(config.num_students)]\n",
    "    student_optimizers = [AdamW(student.parameters(), lr=config.lr) for student in students]\n",
    "    # student_optimizers = [AdamW(student.parameters(), lr=config.lr, weight_decay=0.01) for student in students]\n",
    "  \n",
    "\n",
    "    # Step 4: Fine-tune the router directly using cluster labels\n",
    "    print(\"\\nFine-tuning Router with Hard Cluster Labels and Cosine Annealing:\")\n",
    "    # fine_tune_router_with_hard_labels(\n",
    "    #     router=router,\n",
    "    #     train_loader=train_loader,\n",
    "    #     device=device,\n",
    "    #     big_class_map=big_class_map,\n",
    "    #     epochs=20,  # Adjust epochs as needed\n",
    "    #     lr=config.lr  # Use initial learning rate\n",
    "    # )\n",
    "    \n",
    "    # Load the pre-trained router model\n",
    "    router_path = \"router_epoch20.pth\"\n",
    "    router.load_state_dict(torch.load(router_path))\n",
    "    print(f\"Router loaded from {router_path}\")\n",
    "\n",
    "\n",
    "    # Evaluate the router's accuracy based on cluster labels\n",
    "    # print(\"\\nEvaluating Router Based on Cluster Labels:\")\n",
    "    # evaluate_router_with_cluster_labels(\n",
    "    #     router=router,\n",
    "    #     loader=test_loader,\n",
    "    #     device=device,\n",
    "    #     big_class_map=big_class_map\n",
    "    # )\n",
    "\n",
    "    # print(\"\\nEvaluating Router Cluster-wise Validation Accuracies:\")\n",
    "    # cluster_accuracies = evaluate_router_cluster_accuracies(\n",
    "    #     router=router,\n",
    "    #     loader=test_loader,\n",
    "    #     device=device,\n",
    "    #     big_class_map=big_class_map\n",
    "    # )\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nVisualizing Router Assignments:\")\n",
    "    visualize_router_assignments(router, test_loader, config.num_classes, config.num_students, device)\n",
    "\n",
    "    # # Step 6: Distill teacher knowledge into students\n",
    "    print(\"\\nDistilling Teacher Knowledge into a Single Student:\")\n",
    "    single_student = StudentModel().to(device)\n",
    "    optimizer_student = AdamW(single_student.parameters(), lr=config.lr)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        distill_teacher_to_student(teacher, single_student, train_loader, optimizer_student, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "    # Save the single student model\n",
    "    single_student_save_path = \"student_c1_l0.0625.pth\"\n",
    "    torch.save(single_student.state_dict(), single_student_save_path)\n",
    "    print(f\"Single Student saved to {single_student_save_path}\")\n",
    "\n",
    "    # print(\"\\nDuplicating the Single Student:\")\n",
    "    ##Load the pre-trained single student model\n",
    "    # single_student_path = \"student_1.pth\"  # Ensure this is the correct path to your saved student model\n",
    "\n",
    "    # students = []\n",
    "    # for i in range(config.num_students):\n",
    "    #     student = StudentModel().to(device)  # Initialize a new student model\n",
    "    #     student.load_state_dict(torch.load(single_student_path))  # Load the weights from the single student\n",
    "    #     students.append(student)  # Add the student to the list\n",
    "\n",
    "    # Evaluate the single student on the validation dataset\n",
    "    print(\"\\nEvaluating the Single Student:\")\n",
    "    evaluate_with_metrics(students[0], test_loader, device, description=\"Single Student\")\n",
    "\n",
    "    print(\"\\nDuplicating the Single Student:\")\n",
    "    students = []\n",
    "    for i in range(config.num_students):\n",
    "        student = StudentModel().to(device)\n",
    "        student.load_state_dict(torch.load(single_student_save_path))\n",
    "        students.append(student)\n",
    "\n",
    "    print(f\"{config.num_students} Students initialized by duplicating the Single Student model.\")\n",
    "\n",
    "    # print(\"\\nDistilling Teacher Knowledge into Students:\")\n",
    "    # for i, student in enumerate(students):\n",
    "    #     optimizer_student = AdamW(student.parameters(), lr=config.lr)\n",
    "    #     #for epoch in range(config.epochs // 2):\n",
    "    #     for epoch in range(30):\n",
    "    #         distill_teacher_to_student(teacher, student, train_loader, optimizer_student, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "    #     # Save the distilled student model\n",
    "    #     student_save_path = config.student_model_path.format(i + 1)\n",
    "    #     torch.save(student.state_dict(), student_save_path)\n",
    "    #     print(f\"Student {i + 1} saved to {student_save_path}\")\n",
    "\n",
    "    #     # Evaluate the student on the validation dataset\n",
    "    #     print(f\"\\nEvaluating Student {i + 1}:\")\n",
    "    #     evaluate_with_metrics(student, test_loader, device, description=f\"Student {i + 1}\")\n",
    "\n",
    "    # Step 7: Jointly train the MoE\n",
    "    print(\"\\nJoint Training of Mixture of Experts (MoE):\")\n",
    "    moe_model = MoE(students, router).to(device)\n",
    "    optimizer_moe = AdamW(moe_model.parameters(), lr=config.lr)\n",
    "    # optimizer_moe = AdamW(moe_model.parameters(), lr=config.lr, weight_decay=0.01)\n",
    "    train_moe(moe_model, train_loader, optimizer_moe, nn.CrossEntropyLoss(), device, epochs=config.epochs)\n",
    "\n",
    "    # Step 8: Evaluate the MoE model\n",
    "    print(\"\\nEvaluating Mixture of Experts (MoE):\")\n",
    "    evaluate_with_metrics(moe_model, test_loader, device, description=\"MoE\")\n",
    "\n",
    "    # Step 9: Visualize specialization\n",
    "    print(\"\\nVisualizing Specialization:\")\n",
    "    visualize_specialization(moe_model, test_loader, device, config.num_classes, config.num_students)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
