Distilling Teacher Knowledge into a Single Student:
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Distill Loss: 4.6184, Accuracy: 0.4995
Distill Loss: 3.3804, Accuracy: 0.6603
Distill Loss: 2.9011, Accuracy: 0.7229
Distill Loss: 2.6308, Accuracy: 0.7519
Distill Loss: 2.4830, Accuracy: 0.7694
Distill Loss: 2.3437, Accuracy: 0.7846
Distill Loss: 2.2577, Accuracy: 0.7948
Distill Loss: 2.1641, Accuracy: 0.8057
Distill Loss: 2.1145, Accuracy: 0.8125
Distill Loss: 2.0389, Accuracy: 0.8174
Distill Loss: 1.9716, Accuracy: 0.8256
Distill Loss: 1.9437, Accuracy: 0.8291
Distill Loss: 1.8909, Accuracy: 0.8321
Distill Loss: 1.8648, Accuracy: 0.8350
Distill Loss: 1.8161, Accuracy: 0.8397
Distill Loss: 1.7927, Accuracy: 0.8431
Distill Loss: 1.7848, Accuracy: 0.8435
Distill Loss: 1.7368, Accuracy: 0.8490
Distill Loss: 1.7027, Accuracy: 0.8521
Distill Loss: 1.6822, Accuracy: 0.8552
Distill Loss: 1.6573, Accuracy: 0.8574
Distill Loss: 1.6387, Accuracy: 0.8595
Distill Loss: 1.6091, Accuracy: 0.8619
Distill Loss: 1.5898, Accuracy: 0.8646
Distill Loss: 1.5734, Accuracy: 0.8659
Distill Loss: 1.5712, Accuracy: 0.8657
Distill Loss: 1.5433, Accuracy: 0.8678
Distill Loss: 1.5219, Accuracy: 0.8715
Distill Loss: 1.5112, Accuracy: 0.8705
Distill Loss: 1.5016, Accuracy: 0.8729
Distill Loss: 1.4893, Accuracy: 0.8732
Distill Loss: 1.4736, Accuracy: 0.8756
Distill Loss: 1.4550, Accuracy: 0.8775
Distill Loss: 1.4633, Accuracy: 0.8762
Distill Loss: 1.4511, Accuracy: 0.8772
Distill Loss: 1.4310, Accuracy: 0.8805
Distill Loss: 1.4114, Accuracy: 0.8820
Distill Loss: 1.4068, Accuracy: 0.8826
Distill Loss: 1.3963, Accuracy: 0.8835
Distill Loss: 1.3858, Accuracy: 0.8847
Distill Loss: 1.3809, Accuracy: 0.8850
Distill Loss: 1.3658, Accuracy: 0.8867
Distill Loss: 1.3436, Accuracy: 0.8900
Distill Loss: 1.3559, Accuracy: 0.8881
Distill Loss: 1.3453, Accuracy: 0.8885
Distill Loss: 1.3447, Accuracy: 0.8899
Distill Loss: 1.3263, Accuracy: 0.8895
Distill Loss: 1.3274, Accuracy: 0.8907
Distill Loss: 1.3182, Accuracy: 0.8929
Distill Loss: 1.3171, Accuracy: 0.8933
Distill Loss: 1.3077, Accuracy: 0.8936
Distill Loss: 1.2990, Accuracy: 0.8947
Distill Loss: 1.2909, Accuracy: 0.8944
Distill Loss: 1.2777, Accuracy: 0.8961
Distill Loss: 1.2806, Accuracy: 0.8952
Distill Loss: 1.2711, Accuracy: 0.8963
Distill Loss: 1.2606, Accuracy: 0.8971
Distill Loss: 1.2609, Accuracy: 0.8990
Distill Loss: 1.2600, Accuracy: 0.8974
Distill Loss: 1.2651, Accuracy: 0.8966
Distill Loss: 1.2539, Accuracy: 0.8986
Distill Loss: 1.2474, Accuracy: 0.9017
Distill Loss: 1.2416, Accuracy: 0.8981
Distill Loss: 1.2249, Accuracy: 0.9018
Distill Loss: 1.2175, Accuracy: 0.9013
Distill Loss: 1.2189, Accuracy: 0.9026
Distill Loss: 1.2213, Accuracy: 0.9006
Distill Loss: 1.2208, Accuracy: 0.9022
Distill Loss: 1.1970, Accuracy: 0.9051
Distill Loss: 1.1995, Accuracy: 0.9039
Distill Loss: 1.1918, Accuracy: 0.9059
Distill Loss: 1.2007, Accuracy: 0.9024
Distill Loss: 1.1959, Accuracy: 0.9042
Distill Loss: 1.2000, Accuracy: 0.9060
Distill Loss: 1.1791, Accuracy: 0.9069
Distill Loss: 1.1882, Accuracy: 0.9041
Distill Loss: 1.1810, Accuracy: 0.9071
Distill Loss: 1.1769, Accuracy: 0.9061
Distill Loss: 1.1775, Accuracy: 0.9064
Distill Loss: 1.1736, Accuracy: 0.9056
Distill Loss: 1.1692, Accuracy: 0.9066
Distill Loss: 1.1604, Accuracy: 0.9068
Distill Loss: 1.1623, Accuracy: 0.9075
Distill Loss: 1.1462, Accuracy: 0.9099
Distill Loss: 1.1588, Accuracy: 0.9072
Distill Loss: 1.1498, Accuracy: 0.9106
Distill Loss: 1.1366, Accuracy: 0.9108
Distill Loss: 1.1451, Accuracy: 0.9086
Distill Loss: 1.1472, Accuracy: 0.9096
Distill Loss: 1.1269, Accuracy: 0.9112
Distill Loss: 1.1410, Accuracy: 0.9119
Distill Loss: 1.1345, Accuracy: 0.9110
Distill Loss: 1.1215, Accuracy: 0.9133
Distill Loss: 1.1325, Accuracy: 0.9111
Distill Loss: 1.1218, Accuracy: 0.9127
Distill Loss: 1.1181, Accuracy: 0.9125
Distill Loss: 1.1190, Accuracy: 0.9129
Distill Loss: 1.1135, Accuracy: 0.9142
Distill Loss: 1.1186, Accuracy: 0.9126
Distill Loss: 1.1117, Accuracy: 0.9139
Distill Loss: 1.1051, Accuracy: 0.9156
Distill Loss: 1.1017, Accuracy: 0.9143
Distill Loss: 1.1044, Accuracy: 0.9139
Distill Loss: 1.0991, Accuracy: 0.9151
Distill Loss: 1.0949, Accuracy: 0.9155
Distill Loss: 1.0978, Accuracy: 0.9150
Distill Loss: 1.0924, Accuracy: 0.9157
Distill Loss: 1.0942, Accuracy: 0.9163
Distill Loss: 1.0905, Accuracy: 0.9156
Distill Loss: 1.0950, Accuracy: 0.9140
Distill Loss: 1.0748, Accuracy: 0.9173
Distill Loss: 1.0811, Accuracy: 0.9167
Distill Loss: 1.0874, Accuracy: 0.9165
Distill Loss: 1.0819, Accuracy: 0.9170
Distill Loss: 1.0810, Accuracy: 0.9168
Distill Loss: 1.0790, Accuracy: 0.9176
Distill Loss: 1.0588, Accuracy: 0.9187
Distill Loss: 1.0652, Accuracy: 0.9185
Distill Loss: 1.0761, Accuracy: 0.9164
Distill Loss: 1.0652, Accuracy: 0.9191
Distill Loss: 1.0662, Accuracy: 0.9182
Distill Loss: 1.0729, Accuracy: 0.9173
Distill Loss: 1.0700, Accuracy: 0.9178
Distill Loss: 1.0581, Accuracy: 0.9191
Distill Loss: 1.0567, Accuracy: 0.9193
Distill Loss: 1.0596, Accuracy: 0.9196
Distill Loss: 1.0442, Accuracy: 0.9212
Distill Loss: 1.0510, Accuracy: 0.9200
Distill Loss: 1.0508, Accuracy: 0.9204
Distill Loss: 1.0413, Accuracy: 0.9198
Distill Loss: 1.0502, Accuracy: 0.9197
Distill Loss: 1.0532, Accuracy: 0.9195
Distill Loss: 1.0463, Accuracy: 0.9201
Distill Loss: 1.0396, Accuracy: 0.9211
Distill Loss: 1.0426, Accuracy: 0.9208
Distill Loss: 1.0350, Accuracy: 0.9203
Distill Loss: 1.0441, Accuracy: 0.9210
Distill Loss: 1.0285, Accuracy: 0.9218
Distill Loss: 1.0353, Accuracy: 0.9224
Distill Loss: 1.0351, Accuracy: 0.9207
Distill Loss: 1.0320, Accuracy: 0.9221
Distill Loss: 1.0281, Accuracy: 0.9221
Distill Loss: 1.0127, Accuracy: 0.9239
Distill Loss: 1.0170, Accuracy: 0.9230
Distill Loss: 1.0240, Accuracy: 0.9231
Distill Loss: 1.0198, Accuracy: 0.9228
Distill Loss: 1.0132, Accuracy: 0.9239
Distill Loss: 1.0236, Accuracy: 0.9225
Distill Loss: 1.0166, Accuracy: 0.9240
Distill Loss: 1.0226, Accuracy: 0.9221
Distill Loss: 1.0049, Accuracy: 0.9249
Distill Loss: 1.0128, Accuracy: 0.9242
Distill Loss: 1.0095, Accuracy: 0.9242
Distill Loss: 1.0071, Accuracy: 0.9251
Distill Loss: 1.0093, Accuracy: 0.9233
Distill Loss: 1.0109, Accuracy: 0.9245
Distill Loss: 0.9996, Accuracy: 0.9258
Distill Loss: 0.9929, Accuracy: 0.9262
Distill Loss: 0.9935, Accuracy: 0.9264
Distill Loss: 1.0054, Accuracy: 0.9250
Distill Loss: 0.9941, Accuracy: 0.9253
Distill Loss: 0.9917, Accuracy: 0.9263
Distill Loss: 1.0129, Accuracy: 0.9238
Distill Loss: 0.9982, Accuracy: 0.9256
Distill Loss: 0.9961, Accuracy: 0.9261
Distill Loss: 1.0030, Accuracy: 0.9254
Distill Loss: 0.9960, Accuracy: 0.9259
Distill Loss: 0.9791, Accuracy: 0.9281
Distill Loss: 0.9874, Accuracy: 0.9261
Distill Loss: 0.9840, Accuracy: 0.9276
Distill Loss: 0.9797, Accuracy: 0.9277
Distill Loss: 0.9956, Accuracy: 0.9262
Distill Loss: 0.9830, Accuracy: 0.9271
Distill Loss: 0.9780, Accuracy: 0.9280
Distill Loss: 0.9797, Accuracy: 0.9280
Distill Loss: 0.9850, Accuracy: 0.9268
Distill Loss: 0.9877, Accuracy: 0.9264
Distill Loss: 0.9809, Accuracy: 0.9282
Distill Loss: 0.9817, Accuracy: 0.9272
Distill Loss: 0.9696, Accuracy: 0.9285
Distill Loss: 0.9769, Accuracy: 0.9286
Distill Loss: 0.9722, Accuracy: 0.9283
Distill Loss: 0.9734, Accuracy: 0.9287
Distill Loss: 0.9736, Accuracy: 0.9295
Distill Loss: 0.9704, Accuracy: 0.9281
Distill Loss: 0.9745, Accuracy: 0.9286
Distill Loss: 0.9712, Accuracy: 0.9282
Distill Loss: 0.9735, Accuracy: 0.9288
Distill Loss: 0.9696, Accuracy: 0.9292
Distill Loss: 0.9752, Accuracy: 0.9288
Distill Loss: 0.9620, Accuracy: 0.9293
Distill Loss: 0.9677, Accuracy: 0.9287
Distill Loss: 0.9655, Accuracy: 0.9292
Distill Loss: 0.9698, Accuracy: 0.9286
Distill Loss: 0.9721, Accuracy: 0.9284
Distill Loss: 0.9571, Accuracy: 0.9292
Distill Loss: 0.9610, Accuracy: 0.9286
Distill Loss: 0.9614, Accuracy: 0.9286
Distill Loss: 0.9611, Accuracy: 0.9300
Distill Loss: 0.9500, Accuracy: 0.9308
Single Student saved to big_student_1.pth

Duplicating the Single Student:
/content/create_big_student.py:695: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  student.load_state_dict(torch.load(single_student_path))  # Load the weights from the single student

Evaluating the Single Student:
Unsupported operator aten::max_pool2d encountered 3 time(s)
Single Student Results:
Loss: 0.3780, Accuracy: 0.9042
Latency per Image: 0.001310 secs
FLOPs per Image: 0.61 MFLOPs
4 Students initialized by duplicating the Single Student model.
